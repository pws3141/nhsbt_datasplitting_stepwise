---
title: "<br>How do training–test splits and stepwise selection affect model robustness?"
subtitle: "Some examples using the `GBSG` dataset."
author: 
  - name: Paul Smith
    email: paul.smith9@nhsbt.nhs.uk
date: today
format: 
  html:
    ## Required
    css: "./assets/nhsbt_report_theme.css"
    toc: true
    toc-location: left
    # Title options
    title-block-banner: '#005EB8'
    title-block-banner-color: '#ffffff'
    backgroundcolor: '#f0f4f5'
    # Set font
    mainfont: 'Open Sans'
    ## Optional
    embed-resources: true
    code-annotations: hover
    fig-width: 8
    fig-asp: 0.618
execute:
  freeze: auto
  #echo: false
  #warning: false
include-in-header:
  - text: |
     <link rel = "shortcut icon" href = "./assets/favicon.ico" />
bibliography: ref.bib
---

# Introduction

Here, I'm showing -- using examples -- how splitting the data into training and test sets, and using stepwise variable selection, can (does?) result in non-robust models. 

Data splitting
: where a dataset is split into *training* and *test* samples by a random process. The training sample is used for model development, and the test sample for model validation.

Stepwise variable selection
: selecting variables used in the final model in a stepwise manner. Either starting with the null model and adding predictors one-by-one, or starting with all predictors and removing them.



## Data splitting

The issues with data-splitting are discussed in @harrell2001regression [Section 5.3.3]. These include

1. Data-splitting greatly reduces the sample size for both model development and model testing. 
2. Repeating the process with a different split can result in different assessments of predictive accuracy.
3. It does not validate the final model, only a model developed on a subset of the data.

Number (3) is discussed further [here](https://www.fharrell.com/post/split-val/), specifically:

> When feature selection is done, data splitting validates just one of a myriad of potential models. In effect it validates an example model. Resampling (repeated cross-validation or the bootstrap) validate the process that was used to develop the model. Resampling is honest in reporting the results because it depicts the uncertainty in feature selection, e.g., the disagreements in which variables are selected from one resample to the next.[^note1]

[^note1]: That is, for each bootstrap sample, some sort of variable selection is done (*e.g.* stepwise, or lasso). The proportion of bootstrap samples where each variable is chosen in the model can then be reported to give an idea of the uncertainty in variable selection.

A good primer on the evaluation of clinical prediction models is given by @collins2024evaluation. In this, the authors state,

> Randomly splitting obviously creates two smaller datasets, and often the full dataset is not even large enough to begin with. Having a dataset that is too small to develop the model increases the likelihood of overfitting and producing an unreliable model, and having a test set that is too small will not be able to reliably and precisely estimate model performance—this is a clear waste of precious information.

Even more succinctly, @steyerberg2018validation states,

> random data splitting should be abolished for validation of prediction models.


## Stepwise variable selection

To quote @harrell2001regression[Section 4.3],

> Stepwise variable selection has been a very popular technique for many years, but if this procedure had just been proposed as a statistical method, it would most likely be rejected because it violates every principle of statistical estimation and hypothesis testing.

Some of the issues include:

1. It yields $R^2$ values that are biased high.
2. The $F$ and $\chi^2$ test statistics do not have the claimed distribution.
3. The standard errors of regression coefficient estimates are biased low and confidence intervals for effects and predicted values are falsely narrow.
4. It yields $P$-values that are too small (due to severe multiple comparison problems).
5. The regression coefficients are biased high in absolute value. That is, if $\hat \beta > 0$, $E(\hat \beta | P < 0.05, \hat \beta > 0) > \beta$.

@copas1991estimating explain how (5) occurs, 

> The choice of the variables to be included depends on estimated regression coefficients rather than their true values, and so $X_j$ is more likely to be included if its regression coefficient is over-estimated than if its regression coefficient is underestimated.

To prevent regression coefficients being too large, Tibshirani's^[of *An
Introduction to Statistical Learning* fame.] *lasso* procedure can be used
[@tibshirani1996regression]. This shrinks coefficient estimates towards zero by
including a constraint that the sum of absolute values of the coefficient
estimates must be less than some $k$. Note that the lasso procedure shares many
of stepwise's deficiencies, namely that there is a "low probability of
selecting the 'correct' variables" (see [StackExchange](https://stats.stackexchange.com/questions/411035/does-lasso-suffer-from-the-same-problems-stepwise-regression-does/426805#426805) for some more information).

@derksen1992backward found that when the stepwise procedure was used, the final
model represented noise between $20\%$ and $74\%$ of the time, and the final
model contained less than half of the actual number of authentic predictors. To
see this in action, Frank Harrell has some simulations as part of his
'Regression Modelling Strategies' [course
notes](https://hbiostat.org/rmsc/multivar#sec-multivar-variable-selection). 

# Examples

First, we'll load some packages.

```{r}
#| warning: false

#renv::restore()

library(data.table)    
library(survival)
library(survminer)
library(broom) # tidy
library(mlr3)
library(mlr3viz)
library(mlr3learners)
library(mlr3proba) # survival
library(mlr3fselect) # feature selection
library(lattice)

# number of random splits
iters <- 100
```

and create an `{mlr3}` Task.^[I'm using [mlr3](https://mlr3.mlr-org.com/) for the first example because it makes it easy to do repeated data-splitting and model fitting. For the second example I'll use the survival package directly, as it allows us to do stepwise variable selection.]

```{r task}
task = tsk("gbsg")
task
```

In the first example, I will look at the variation in the outputted estimated coefficients and C-statistics from `r iters` random training-test splits^[That is, 70\% of the observations will be randomly allocate to the training set, and the remainder to the test set. The model will be built on the training set and validated (*i.e.* C-statistic calculated) on the test set.]

The second example is similar to the first, but with stepwise variable selection included in the model building step. This means that the predictors included in the model will change depending on the observations included in the training split.

:::{.callout-note collapse="true"}
# Building the Cox PH model on a single training sample

Before we build `r iters` models on `r iters` random training sets, lets look at a single model, the coefficient point estimates, and their confidence intervals.

Building the model:
```{r}
set.seed(16)

# split the data into a random 70% training set, and 30% test set
split = partition(task, ratio = 0.7)
# create the Learner: Cox PH model
cox = lrn("surv.coxph")
# build the model
cox$train(task, row_ids = split$train)
```

The hazard ratio estimates are shown in @fig-single-hr.

```{r}
#| fig-cap: "Hazard ratio estimates for a model built on a single 70% training set."
#| label: fig-single-hr

# plot the hazard ratio estimates
autoplot(cox)
```

The C-statistic we get from this model when applied to the (unseen) test set is:
```{r}
# create predictions on test set
preds = cox$predict(task, row_ids = split$test)
# C-statistic of predictions
preds$score(msr("surv.cindex"))
```

:::

## Data splitting

```{r modelfitting}
#| output: false 

set.seed(1)

# using mlr3 syntax:

# Create the Learner: Cox PH model
learner = lrn("surv.coxph")

# Benchmark grid: 100 different training-test sets
holdouts = rsmps(rep("holdout", iters), ratio = 0.7)
design = benchmark_grid(task, learner, holdouts)

# build the models
bmr = benchmark(design, store_models = TRUE)
```


Now `r iters` models have been built, let's look at the hazard ratio estimates. Each dot in @fig-full-coefficients is one coefficient estimate. We can compare the spread of these estimates to the confidence intervals given in @fig-single-hr -- we see that they are similar. However, two points:

1. When point estimates are used directly -- for example, in donor risk indices -- it is important to illustrate how the particular set of coefficient estimates affects the resulting score (e.g. the DRI value).
2. The confidence intervals in @fig-single-hr are univariate, whereas the hazard ratio estimates are often correlated (for instance, if `grade3` is higher than expected, `grade2` may be lower). Small correlated shifts in several coefficients can therefore have a large combined effect on model predictions. This joint behaviour is not visible in @fig-single-hr but becomes evident through repeated sampling or sensitivity analysis.


```{r params}
# estimated (donor) parameters

bmrdt <- as.data.table(bmr)
learners <- bmrdt[, learner]
coefs <- rbindlist(
  lapply(learners, \(m) as.list(m$model$coefficients))
)

# plot
# reshape to long format
coef_long = melt(coefs, 
                 measure.vars = names(coefs), 
                 variable.name = "term", 
                 value.name = "estimate")

```

```{r}
#| fig-cap: Strip plot showing the hazard ratio estimates from models built on the 100 different training sets.
#| label: fig-full-coefficients


stripplot(term ~ exp(estimate), data = coef_long,
        xlab = "Hazard ratio estimates", ylab = "")
```

The associated C-statistics calculated on the test observation are given in @fig-full-cstats

```{r}
#| fig-cap: Density plot of the C-statistics calcualcted on the 100 test sets.
#| label: fig-full-cstats

# C-statistic
acc = bmr$score(msr("surv.cindex"))
summary(acc[, c(surv.cindex)])
densityplot(~ surv.cindex, data = acc)
```

So, even with a comprehensive SAP, there is not much stopping a researcher from choosen the model that results in the "best" C-statistic when applied to that particular test set.
Here, we could^[If we really wanted to do some bad statistical practice.] pick the model built on the `r acc[, which.max(surv.cindex)]`$^{th}$ training-test split, as this gave a C-statistic of `r round(acc[, max(surv.cindex)], 4)`.


## Data splitting with stepwise selection

In this section, in each training-test split, the model is built on the training set using stepwise variable selection, starting with the full model and adding/ removing variables based on the AIC.

```{r}
# load data
gbsg <- as.data.table(mlr3::tsk("gbsg")$data())

n_x <- nrow(gbsg)
n1 <- 0.7 # size of training set
```

First, create a data.table where each row consists of a model `.id`, and then the `floor(0.7 x n) =` `r floor(n1 * n_x)` indices which are included in the `.id`$^{th}$ training set.

```{r}
set.seed(1)
training_idx <- rbindlist(
  lapply(seq_len(iters), function(i) {
    idx <- sample(1:n_x, floor(n1 * n_x))
    data.frame(.id = i, as.list(idx))
  }
  ),
  use.names = FALSE
)

# change column names except '.id'
old_names <- names(training_idx)
new_names <- c(".id", paste0("idx_", seq_len(ncol(training_idx) - 1))) 
setnames(training_idx, old = old_names, new = new_names)

training_idx[1:5, 1:12]
```

Now, we build the `r iters` models on the random training sets, and calculate the associated C-statistic on the test set. Note that, unlike the previous example, here the variables chosen in each model could be different between models.

```{r}
# split data into training and test sets
# and obtain cox coefs and C-statistics
models <- training_idx[, {
  tmp_train <- gbsg[unlist(.SD)]
  tmp_test <- gbsg[-unlist(.SD)]
  # fit the full model to the training data
  cox_full_tmp <- coxph(Surv(time, status) ~ .,
                        data = tmp_train)
  # use stepwise selection on training data
  cox_step_tmp <- step(cox_full_tmp,
                       direction = "both",
                       trace = 0)
  # get linear predictor on test data
  lp_tmp <- predict(cox_step_tmp, 
                    newdata = tmp_test, type = "lp")
  # obtain c-statistic using linear predictor
  c_stat <- concordance(Surv(tmp_test$time, tmp_test$status) ~ lp_tmp, 
                        reverse = TRUE) 
  data.table(as.data.table(tidy(cox_step_tmp)),
             c = c_stat$concordance)
}, by = .id] 
```

@fig-stepwise-models gives the proportion that each model is output using stepwise variable selection. Overall, there are 16 different models created from applying the stepwise procedure to the `r iters` random training sets.

```{r}
#| fig-cap: Proportion that each model is chosen using the stepwise variable selection procedure.
#| label: fig-stepwise-models

# Model factors:
# predictors in model
model_variables <- models[, .(
  terms_string = paste(
    unique(sub("[0-9]+$", "", term)),
    collapse = ", "
  )
), by = .id]

#model_variables

proportion_model_variables <- model_variables[
  , .(prop = nrow(.SD) / nrow(model_variables))
  , by = terms_string]

#proportion_model_variables
proportion_model_variables[,
           terms_string := factor(terms_string, levels = terms_string[order(prop)])]  

barchart(terms_string ~ prop, proportion_model_variables)
```

The coefficient estimates (when the variables were chosen to be in a model) are shown in @fig-stepwise-hr.

```{r}
#| fig-cap: Hazard ratio estimates for the models built using stepwise selection on the 100 different training sets.
#| label: fig-stepwise-hr

# coefficient estimates (donor factors)
stripplot(term ~ exp(estimate), data = models, 
          xlab = "Hazard ratio estimate", ylab = "")

```

The C-statistics calculated on the test sets associated with the `r iters` different models are shown in @fig-stepwise-cstats.

```{r}
#| label: fig-stepwise-cstats
#| fig-cap: Density plot of the C-statistics calcualcted on the 100 test sets.

# C-statistics
c_stats <- models[, .(cstat = unique(c)), by = .id]
#c_stats[order(cstat, decreasing = TRUE)]
summary(c_stats[, cstat])
densityplot(~ cstat, data = c_stats)
```

# Conclusion

Here, I've shown, via simulation, the effect on model building and validation from splitting the data, building the model on the training set and validating using the test set. There are a few things to consider:

1. Uncertainty should be reported: the [Code of Practice for Statistics](https://code.statisticsauthority.gov.uk/) states that "bias and uncertainty should be identified and explained to users" (Q2.4). If we are building a DRI using point estimates, this is especially important, as we need to explain why *that particular training set and model* was chosen (if data-splitting and stepwise selection were used).
2. The large variance of the final model chosen via the stepwise selection process is not just a result of data-splitting but is inherent in the procedure. This can be simulated via bootstrapping the full sample (discussed in Frank Harrell's [course
notes](https://hbiostat.org/rmsc/multivar#sec-multivar-variable-selection)).
3. Model building and validation techniques that are currently used need to be justifiable and justified -- relying on historical precedence isn't enough.
    - Data splitting leads to poor performing prediction models [see @steyerberg2018validation; @collins2024evaluation].
    - Stepwise selection violates many of the assumptions underlying the final model, resulting in standard errors that are too low, narrow confidence intervals, and a increase in false-positive results. In addition, the coefficient estimates are biased high when chosen in the model [@harrell2001regression].

As discussed in @collins2024evaluation and @harrell2001regression (and plenty of other places), building and validating the model on the full dataset is preferable to data splitting. Bootstrapping is required for the validation step, to be able to estimate (and remove) the bias in the validation measure -- a result of building and validating the model on the same data. An example of this technique is given in @steyerberg2003internal.

@efthimiou2024developing recommend that we "should identify potential predictors based on the literature review and expert knowledge", and additionally state,

> Stepwise methods for variable selection ... are not recommended because they might lead to bias in estimation and worse predictive performance.

# Fin
 
  